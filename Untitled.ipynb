{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyfolio as pf\n",
    "from collections import OrderedDict\n",
    "import sklearn.covariance\n",
    "\n",
    "import cvxopt as opt\n",
    "from cvxopt import blas, solvers\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "# Turn off progress printing \n",
    "solvers.options['show_progress'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to estimate robust covariance and correlation matrices\n",
    "\n",
    "def cov2cor(X):\n",
    "    D = np.zeros_like(X)\n",
    "    d = np.sqrt(np.diag(X))\n",
    "    np.fill_diagonal(D, d)\n",
    "    DInv = np.linalg.inv(D)\n",
    "    R = np.dot(np.dot(DInv, X), DInv)\n",
    "    return R\n",
    "\n",
    "def cov_robust(X):\n",
    "    oas = sklearn.covariance.OAS()\n",
    "    oas.fit(X)\n",
    "    return pd.DataFrame(oas.covariance_, index=X.columns, columns=X.columns)\n",
    "    \n",
    "def corr_robust(X):\n",
    "    cov = cov_robust(X).values\n",
    "    shrunk_corr = cov2cor(cov)\n",
    "    return pd.DataFrame(shrunk_corr, index=X.columns, columns=X.columns)\n",
    "\n",
    "def is_pos_def(x):\n",
    "    return np.all(np.linalg.eigvals(x) > 0)\n",
    "\n",
    "def mean_variance(returns, cov=None, shrink_means=False):\n",
    "    n = len(returns)\n",
    "    returns = np.asmatrix(returns)\n",
    "    \n",
    "    N = 50\n",
    "    mus = [10**(5.0 * t/N - 1.0) for t in range(N)]\n",
    "    \n",
    "    # Convert to cvxopt matrices\n",
    "    if cov is None:\n",
    "        S = opt.matrix(np.cov(returns))\n",
    "    else:\n",
    "        S = opt.matrix(cov)\n",
    "    \n",
    "    if shrink_means:\n",
    "        pbar = opt.matrix(np.ones(cov.shape[0]))\n",
    "    else:\n",
    "        pbar = opt.matrix(np.mean(returns, axis=1))\n",
    "    \n",
    "    # Create constraint matrices\n",
    "    G = -opt.matrix(np.eye(n))   # negative n x n identity matrix\n",
    "    h = opt.matrix(0.0, (n ,1))\n",
    "    A = opt.matrix(1.0, (1, n))\n",
    "    b = opt.matrix(1.0)\n",
    "    \n",
    "    # Calculate efficient frontier weights using quadratic programming\n",
    "    portfolios = [solvers.qp(mu*S, -pbar, G, h, A, b)['x'] \n",
    "                  for mu in mus]\n",
    "    ## CALCULATE RISKS AND RETURNS FOR FRONTIER\n",
    "    returns = [blas.dot(pbar, x) for x in portfolios]\n",
    "    risks = [np.sqrt(blas.dot(x, S*x)) for x in portfolios]\n",
    "    ## CALCULATE THE 2ND DEGREE POLYNOMIAL OF THE FRONTIER CURVE\n",
    "    m1 = np.polyfit(returns, risks, 2)\n",
    "    x1 = np.sqrt(m1[2] / m1[0])\n",
    "    # CALCULATE THE OPTIMAL PORTFOLIO\n",
    "    wt = solvers.qp(opt.matrix(x1 * S), -pbar, G, h, A, b)['x']\n",
    "    return np.asarray(wt)\n",
    "\n",
    "def get_mean_variance(returns, cov):\n",
    "    try: \n",
    "        w = mean_variance(returns.values, cov=cov.values)[:, 0]\n",
    "    except:\n",
    "        w = np.empty(cov.shape[0])\n",
    "        w[:] = np.nan\n",
    "        \n",
    "    return w\n",
    "\n",
    "def get_min_variance(returns, cov):\n",
    "    try: \n",
    "        w = mean_variance(returns.values, cov=cov.values, shrink_means=True)[:, 0]\n",
    "    except:\n",
    "        w = np.empty(cov.shape[0])\n",
    "        w[:] = np.nan\n",
    "        \n",
    "    return w\n",
    "\n",
    "def tail_ratio(returns):\n",
    "    \"\"\"Determines the ratio between the right (95%) and left tail (5%).\n",
    "    For example, a ratio of 0.25 means that losses are four times\n",
    "    as bad as profits.\n",
    "    Parameters\n",
    "    ----------\n",
    "    returns : pd.Series\n",
    "        Daily returns of the strategy, noncumulative.\n",
    "         - See full explanation in tears.create_full_tear_sheet.\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        tail ratio\n",
    "    \"\"\"\n",
    "\n",
    "    return np.abs(np.percentile(returns, 95)) / \\\n",
    "        np.abs(np.percentile(returns, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# On 20130210, v0.2\n",
    "# Critical Line Algorithm\n",
    "# by MLdP <lopezdeprado@lbl.gov>\n",
    "import numpy as np\n",
    "#---------------------------------------------------------------\n",
    "#---------------------------------------------------------------\n",
    "class CLA:\n",
    "    def __init__(self,mean,covar,lB,uB):\n",
    "        # Initialize the class\n",
    "        if (mean==np.ones(mean.shape)*mean.mean()).all():mean[-1,0]+=1e-5\n",
    "        self.mean=mean\n",
    "        self.covar=covar\n",
    "        self.lB=lB\n",
    "        self.uB=uB\n",
    "        self.w=[] # solution\n",
    "        self.l=[] # lambdas\n",
    "        self.g=[] # gammas\n",
    "        self.f=[] # free weights\n",
    "#---------------------------------------------------------------\n",
    "    def solve(self):\n",
    "        # Compute the turning points,free sets and weights\n",
    "        f,w=self.initAlgo()\n",
    "        self.w.append(np.copy(w)) # store solution\n",
    "        self.l.append(None)\n",
    "        self.g.append(None)\n",
    "        self.f.append(f[:])\n",
    "        while True:\n",
    "            #1) case a): Bound one free weight\n",
    "            l_in=None\n",
    "            if len(f)>1:\n",
    "                covarF,covarFB,meanF,wB=self.getMatrices(f)\n",
    "                covarF_inv=np.linalg.inv(covarF)\n",
    "                j=0\n",
    "                for i in f:\n",
    "                    l,bi=self.computeLambda(covarF_inv,covarFB,meanF,wB,j,[self.lB[i],self.uB[i]])\n",
    "                    if l>l_in:l_in,i_in,bi_in=l,i,bi\n",
    "                    j+=1\n",
    "            #2) case b): Free one bounded weight\n",
    "            l_out=None\n",
    "            if len(f)<self.mean.shape[0]:\n",
    "                b=self.getB(f)\n",
    "                for i in b:\n",
    "                    covarF,covarFB,meanF,wB=self.getMatrices(f+[i])\n",
    "                    covarF_inv=np.linalg.inv(covarF)\n",
    "                    l,bi=self.computeLambda(covarF_inv,covarFB,meanF,wB,meanF.shape[0]-1, \\\n",
    "                        self.w[-1][i])\n",
    "                    if (self.l[-1]==None or l<self.l[-1]) and l>l_out:l_out,i_out=l,i                \n",
    "            if (l_in==None or l_in<0) and (l_out==None or l_out<0):\n",
    "                #3) compute minimum variance solution\n",
    "                self.l.append(0)\n",
    "                covarF,covarFB,meanF,wB=self.getMatrices(f)\n",
    "                covarF_inv=np.linalg.inv(covarF)\n",
    "                meanF=np.zeros(meanF.shape)\n",
    "            else:\n",
    "                #4) decide lambda\n",
    "                if l_in>l_out:\n",
    "                    self.l.append(l_in)\n",
    "                    f.remove(i_in)\n",
    "                    w[i_in]=bi_in # set value at the correct boundary\n",
    "                else:\n",
    "                    self.l.append(l_out)\n",
    "                    f.append(i_out)\n",
    "                covarF,covarFB,meanF,wB=self.getMatrices(f)\n",
    "                covarF_inv=np.linalg.inv(covarF)\n",
    "            #5) compute solution vector\n",
    "            wF,g=self.computeW(covarF_inv,covarFB,meanF,wB)\n",
    "            for i in range(len(f)):w[f[i]]=wF[i]\n",
    "            self.w.append(np.copy(w)) # store solution\n",
    "            self.g.append(g)\n",
    "            self.f.append(f[:])\n",
    "            if self.l[-1]==0:break\n",
    "        #6) Purge turning points\n",
    "        self.purgeNumErr(10e-10)\n",
    "        self.purgeExcess()\n",
    "#---------------------------------------------------------------    \n",
    "    def initAlgo(self):\n",
    "        # Initialize the algo\n",
    "        #1) Form structured array\n",
    "        a=np.zeros((self.mean.shape[0]),dtype=[('id',int),('mu',float)])\n",
    "        b=[self.mean[i][0] for i in range(self.mean.shape[0])] # dump array into list\n",
    "        a[:]=zip(range(self.mean.shape[0]),b) # fill structured array\n",
    "        #2) Sort structured array\n",
    "        b=np.sort(a,order='mu')\n",
    "        #3) First free weight\n",
    "        i,w=b.shape[0],np.copy(self.lB)\n",
    "        while sum(w)<1:\n",
    "            i-=1\n",
    "            w[b[i][0]]=self.uB[b[i][0]]\n",
    "        w[b[i][0]]+=1-sum(w)\n",
    "        return [b[i][0]],w\n",
    "#---------------------------------------------------------------    \n",
    "    def computeBi(self,c,bi):\n",
    "        if c>0:\n",
    "            bi=bi[1][0]\n",
    "        if c<0:\n",
    "            bi=bi[0][0]\n",
    "        return bi\n",
    "#---------------------------------------------------------------\n",
    "    def computeW(self,covarF_inv,covarFB,meanF,wB):\n",
    "        #1) compute gamma\n",
    "        onesF=np.ones(meanF.shape)\n",
    "        g1=np.dot(np.dot(onesF.T,covarF_inv),meanF)\n",
    "        g2=np.dot(np.dot(onesF.T,covarF_inv),onesF)\n",
    "        if wB==None:\n",
    "            g,w1=float(-self.l[-1]*g1/g2+1/g2),0\n",
    "        else:\n",
    "            onesB=np.ones(wB.shape)\n",
    "            g3=np.dot(onesB.T,wB)\n",
    "            g4=np.dot(covarF_inv,covarFB)\n",
    "            w1=np.dot(g4,wB)\n",
    "            g4=np.dot(onesF.T,w1)\n",
    "            g=float(-self.l[-1]*g1/g2+(1-g3+g4)/g2)\n",
    "        #2) compute weights\n",
    "        w2=np.dot(covarF_inv,onesF)\n",
    "        w3=np.dot(covarF_inv,meanF)\n",
    "        return -w1+g*w2+self.l[-1]*w3,g\n",
    "#---------------------------------------------------------------\n",
    "    def computeLambda(self,covarF_inv,covarFB,meanF,wB,i,bi):\n",
    "        #1) C\n",
    "        onesF=np.ones(meanF.shape)\n",
    "        c1=np.dot(np.dot(onesF.T,covarF_inv),onesF)\n",
    "        c2=np.dot(covarF_inv,meanF)\n",
    "        c3=np.dot(np.dot(onesF.T,covarF_inv),meanF)\n",
    "        c4=np.dot(covarF_inv,onesF)\n",
    "        c=-c1*c2[i]+c3*c4[i]\n",
    "        if c==0:return None,None\n",
    "        #2) bi\n",
    "        if type(bi)==list:bi=self.computeBi(c,bi)\n",
    "        #3) Lambda\n",
    "        if wB==None:\n",
    "            # All free assets\n",
    "            return float((c4[i]-c1*bi)/c),bi\n",
    "        else:\n",
    "            onesB=np.ones(wB.shape)\n",
    "            l1=np.dot(onesB.T,wB)\n",
    "            l2=np.dot(covarF_inv,covarFB)\n",
    "            l3=np.dot(l2,wB)\n",
    "            l2=np.dot(onesF.T,l3)\n",
    "            return float(((1-l1+l2)*c4[i]-c1*(bi+l3[i]))/c),bi\n",
    "#---------------------------------------------------------------\n",
    "    def getMatrices(self,f):\n",
    "        # Slice covarF,covarFB,covarB,meanF,meanB,wF,wB\n",
    "        covarF=self.reduceMatrix(self.covar,f,f)\n",
    "        meanF=self.reduceMatrix(self.mean,f,[0])\n",
    "        b=self.getB(f)\n",
    "        covarFB=self.reduceMatrix(self.covar,f,b)\n",
    "        wB=self.reduceMatrix(self.w[-1],b,[0])\n",
    "        return covarF,covarFB,meanF,wB\n",
    "#---------------------------------------------------------------\n",
    "    def getB(self,f):\n",
    "        return self.diffLists(range(self.mean.shape[0]),f)\n",
    "#---------------------------------------------------------------\n",
    "    def diffLists(self,list1,list2):\n",
    "        return list(set(list1)-set(list2))\n",
    "#---------------------------------------------------------------\n",
    "    def reduceMatrix(self,matrix,listX,listY):\n",
    "        # Reduce a matrix to the provided list of rows and columns\n",
    "        matrix = np.asarray(matrix)\n",
    "        if len(listX)==0 or len(listY)==0:return\n",
    "        matrix_=matrix[:,listY[0]:listY[0]+1]\n",
    "        for i in listY[1:]:\n",
    "            a=matrix[:,i:i+1]\n",
    "            matrix_=np.append(matrix_,a,1) # gets stuck\n",
    "        matrix__=matrix_[listX[0]:listX[0]+1,:]\n",
    "        for i in listX[1:]:\n",
    "            a=matrix_[i:i+1,:]\n",
    "            matrix__=np.append(matrix__,a,0)\n",
    "        return matrix__\n",
    "#---------------------------------------------------------------    \n",
    "    def purgeNumErr(self,tol):\n",
    "        # Purge violations of inequality constraints (associated with ill-conditioned covar matrix)\n",
    "        i=0\n",
    "        while True:\n",
    "            flag=False\n",
    "            if i==len(self.w):break\n",
    "            if abs(sum(self.w[i])-1)>tol:\n",
    "                flag=True\n",
    "            else:\n",
    "                for j in range(self.w[i].shape[0]):\n",
    "                    if self.w[i][j]-self.lB[j]<-tol or self.w[i][j]-self.uB[j]>tol:\n",
    "                        flag=True;break\n",
    "            if flag==True:\n",
    "                del self.w[i]\n",
    "                del self.l[i]\n",
    "                del self.g[i]\n",
    "                del self.f[i]\n",
    "            else:\n",
    "                i+=1\n",
    "        return\n",
    "#---------------------------------------------------------------    \n",
    "    def purgeExcess(self):\n",
    "        # Remove violations of the convex hull\n",
    "        i,repeat=0,False\n",
    "        while True:\n",
    "            if repeat==False:i+=1\n",
    "            if i==len(self.w)-1:break\n",
    "            w=self.w[i]\n",
    "            mu=np.dot(w.T,self.mean)[0,0]\n",
    "            j,repeat=i+1,False\n",
    "            while True:\n",
    "                if j==len(self.w):break\n",
    "                w=self.w[j]\n",
    "                mu_=np.dot(w.T,self.mean)[0,0]\n",
    "                if mu<mu_:\n",
    "                    del self.w[i]\n",
    "                    del self.l[i]\n",
    "                    del self.g[i]\n",
    "                    del self.f[i]\n",
    "                    repeat=True\n",
    "                    break\n",
    "                else:\n",
    "                    j+=1\n",
    "        return\n",
    "#---------------------------------------------------------------\n",
    "    def getMinVar(self):\n",
    "        # Get the minimum variance solution\n",
    "        var=[]\n",
    "        for w in self.w:\n",
    "            a=np.dot(np.dot(w.T,self.covar),w)\n",
    "            var.append(a)\n",
    "        return min(var)**.5,self.w[var.index(min(var))]\n",
    "#---------------------------------------------------------------\n",
    "    def getMaxSR(self):\n",
    "        # Get the max Sharpe ratio portfolio\n",
    "        #1) Compute the local max SR portfolio between any two neighbor turning points\n",
    "        w_sr,sr=[],[]\n",
    "        for i in range(len(self.w)-1):\n",
    "            w0=np.copy(self.w[i])\n",
    "            w1=np.copy(self.w[i+1])\n",
    "            kargs={'minimum':False,'args':(w0,w1)}\n",
    "            a,b=self.goldenSection(self.evalSR,0,1,**kargs)\n",
    "            w_sr.append(a*w0+(1-a)*w1)\n",
    "            sr.append(b)\n",
    "        return max(sr),w_sr[sr.index(max(sr))]\n",
    "#---------------------------------------------------------------\n",
    "    def evalSR(self,a,w0,w1):\n",
    "        # Evaluate SR of the portfolio within the convex combination\n",
    "        w=a*w0+(1-a)*w1\n",
    "        b=np.dot(w.T,self.mean)[0,0]\n",
    "        c=np.dot(np.dot(w.T,self.covar),w)[0,0]**.5\n",
    "        return b/c\n",
    "#---------------------------------------------------------------\n",
    "    def goldenSection(self,obj,a,b,**kargs):\n",
    "        # Golden section method. Maximum if kargs['minimum']==False is passed \n",
    "        from math import log,ceil\n",
    "        tol,sign,args=1.0e-9,1,None\n",
    "        if 'minimum' in kargs and kargs['minimum']==False:sign=-1\n",
    "        if 'args' in kargs:args=kargs['args']\n",
    "        numIter=int(ceil(-2.078087*log(tol/abs(b-a))))\n",
    "        r=0.618033989\n",
    "        c=1.0-r\n",
    "        # Initialize\n",
    "        x1=r*a+c*b;x2=c*a+r*b\n",
    "        f1=sign*obj(x1,*args);f2=sign*obj(x2,*args)\n",
    "        # Loop\n",
    "        for i in range(numIter):\n",
    "            if f1>f2:\n",
    "                a=x1\n",
    "                x1=x2;f1=f2\n",
    "                x2=c*a+r*b;f2=sign*obj(x2,*args)\n",
    "            else:\n",
    "                b=x2\n",
    "                x2=x1;f2=f1\n",
    "                x1=r*a+c*b;f1=sign*obj(x1,*args)\n",
    "        if f1<f2:return x1,sign*f1\n",
    "        else:return x2,sign*f2\n",
    "#---------------------------------------------------------------\n",
    "    def efFrontier(self,points):\n",
    "        # Get the efficient frontier\n",
    "        mu,sigma,weights=[],[],[]\n",
    "        a=np.linspace(0,1,points/len(self.w))[:-1] # remove the 1, to avoid duplications\n",
    "        b=range(len(self.w)-1)\n",
    "        for i in b:\n",
    "            w0,w1=self.w[i],self.w[i+1]\n",
    "            if i==b[-1]:a=np.linspace(0,1,points/len(self.w)) # include the 1 in the last iteration\n",
    "            for j in a:\n",
    "                w=w1*j+(1-j)*w0\n",
    "                weights.append(np.copy(w))\n",
    "                mu.append(np.dot(w.T,self.mean)[0,0])\n",
    "                sigma.append(np.dot(np.dot(w.T,self.covar),w)[0,0]**.5)\n",
    "        return mu,sigma,weights\n",
    "#---------------------------------------------------------------\n",
    "#---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as sch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def getIVP(cov, **kargs):\n",
    "    # Compute the inverse-variance portfolio\n",
    "    ivp = 1. / np.diag(cov)\n",
    "    ivp /= ivp.sum()\n",
    "    return ivp\n",
    "\n",
    "def getClusterVar(cov,cItems):\n",
    "    # Compute variance per cluster\n",
    "    cov_=cov.loc[cItems,cItems] # matrix slice\n",
    "    w_=getIVP(cov_).reshape(-1,1)\n",
    "    cVar=np.dot(np.dot(w_.T,cov_),w_)[0,0]\n",
    "    return cVar\n",
    "\n",
    "def getQuasiDiag(link):\n",
    "    # Sort clustered items by distance\n",
    "    link = link.astype(int)\n",
    "    sortIx = pd.Series([link[-1, 0], link[-1, 1]])\n",
    "    numItems = link[-1, 3]  # number of original items\n",
    "    while sortIx.max() >= numItems:\n",
    "        sortIx.index = range(0, sortIx.shape[0] * 2, 2)  # make space\n",
    "        df0 = sortIx[sortIx >= numItems]  # find clusters\n",
    "        i = df0.index\n",
    "        j = df0.values - numItems\n",
    "        sortIx[i] = link[j, 0]  # item 1\n",
    "        df0 = pd.Series(link[j, 1], index=i + 1)\n",
    "        sortIx = sortIx.append(df0)  # item 2\n",
    "        sortIx = sortIx.sort_index()  # re-sort\n",
    "        sortIx.index = range(sortIx.shape[0])  # re-index\n",
    "\n",
    "    return sortIx.tolist()\n",
    "\n",
    "\n",
    "def getRecBipart(cov, sortIx):\n",
    "    # Compute HRP alloc\n",
    "    w = pd.Series(1, index=sortIx)\n",
    "    cItems = [sortIx]  # initialize all items in one cluster\n",
    "    while len(cItems) > 0:\n",
    "        cItems = [i[j:k] for i in cItems for j, k in ((0, len(i) / 2),\n",
    "                                                      (len(i) / 2, len(i))) if len(i) > 1]  # bi-section\n",
    "        for i in xrange(0, len(cItems), 2):  # parse in pairs\n",
    "            cItems0 = cItems[i]  # cluster 1\n",
    "            cItems1 = cItems[i + 1]  # cluster 2\n",
    "            cVar0 = getClusterVar(cov, cItems0)\n",
    "            cVar1 = getClusterVar(cov, cItems1)\n",
    "            alpha = 1 - cVar0 / (cVar0 + cVar1)\n",
    "            w[cItems0] *= alpha  # weight 1\n",
    "            w[cItems1] *= 1 - alpha  # weight 2\n",
    "    return w\n",
    "\n",
    "\n",
    "def correlDist(corr):\n",
    "    # A distance matrix based on correlation, where 0<=d[i,j]<=1\n",
    "    # This is a proper distance metric\n",
    "    dist = ((1 - corr) / 2.)**.5  # distance matrix\n",
    "    return dist\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generateData(nObs, sLength, size0, size1, mu0, sigma0, sigma1F):\n",
    "    # Time series of correlated variables\n",
    "    # 1) generate random uncorrelated data\n",
    "    x = np.random.normal(mu0, sigma0, size=(nObs, size0))\n",
    "    # each row is a variable\n",
    "    # 2) create correlation between the variables\n",
    "    cols = [random.randint(0, size0 - 1) for i in xrange(size1)]\n",
    "    y = x[:, cols] + np.random.normal(0, sigma0 * sigma1F, size=(nObs, len(cols)))\n",
    "    x = np.append(x, y, axis=1)\n",
    "    # 3) add common random shock\n",
    "    point = np.random.randint(sLength, nObs - 1, size=2)\n",
    "    x[np.ix_(point, [cols[0], size0])] = np.array([[-.5, -.5], [2, 2]])\n",
    "    # 4) add specific random shock\n",
    "    point = np.random.randint(sLength, nObs - 1, size=2)\n",
    "\n",
    "    x[point, cols[-1]] = np.array([-.5, 2])\n",
    "\n",
    "    return x, cols\n",
    "\n",
    "\n",
    "def getHRP(cov, corr):\n",
    "    # Construct a hierarchical portfolio\n",
    "    corr, cov = pd.DataFrame(corr), pd.DataFrame(cov)\n",
    "    dist = correlDist(corr)\n",
    "    link = sch.linkage(dist, 'single')\n",
    "    sortIx = getQuasiDiag(link)\n",
    "    sortIx = corr.index[sortIx].tolist()\n",
    "    # recover labels\n",
    "    hrp = getRecBipart(cov, sortIx)\n",
    "\n",
    "    return hrp.sort_index()\n",
    "\n",
    "\n",
    "def getCLA(cov, **kargs):\n",
    "    # Compute CLA's minimum variance portfolio\n",
    "    mean = np.arange(cov.shape[0]).reshape(-1, 1)\n",
    "    # Not used by C portf\n",
    "    lB = np.zeros(mean.shape)\n",
    "    uB = np.ones(mean.shape)\n",
    "    cla = CLA(mean, cov, lB, uB)\n",
    "    cla.solve()\n",
    "    return cla.w[-1].flatten()\n",
    "\n",
    "\n",
    "def hrpMC(numIters=10000, nObs=520, size0=5, size1=5, mu0=0, sigma0=1e-2,\n",
    "          sigma1F=.25, sLength=260, rebal=22):\n",
    "    # Monte Carlo experiment on HRP\n",
    "    methods = {'getHRP': getHRP, 'getIVP': getIVP, 'getCLA': getCLA}\n",
    "    stats = {k: pd.Series() for k in methods.keys()}\n",
    "    \n",
    "    pointers = range(sLength, nObs, rebal)\n",
    "    for numIter in xrange(int(numIters)):\n",
    "        # print numIter\n",
    "        # 1) Prepare data for one experiment\n",
    "        x, cols = generateData(nObs, sLength, size0,\n",
    "                               size1, mu0, sigma0, sigma1F)\n",
    "        r = pd.DataFrame(columns=[methods.keys()],\n",
    "                         index=range(sLength, nObs))#{i.__name__: pd.Series() for i in methods}\n",
    "        #print r\n",
    "        # 2) Compute portfolios in-sample\n",
    "        for pointer in pointers:\n",
    "            x_ = x[pointer - sLength:pointer]\n",
    "            cov_ = np.cov(x_, rowvar=0)\n",
    "            corr_ = np.corrcoef(x_, rowvar=0)\n",
    "            # 3) Compute performance out-of-sample\n",
    "            x_ = x[pointer:pointer + rebal]\n",
    "            for name, func in methods.iteritems():\n",
    "                w_ = func(cov=cov_, corr=corr_)\n",
    "                # callback\n",
    "                #r_ = pd.Series(np.dot(x_, w_))\n",
    "                #print r[name].append(r_)\n",
    "                #print pointer\n",
    "                r.loc[pointer:pointer + rebal - 1, name] = np.dot(x_, w_)\n",
    "\n",
    "        # 4) Evaluate and store results\n",
    "        for name, func in methods.iteritems():\n",
    "            r_ = r[name].reset_index(drop=True)\n",
    "            p_ = (1 + r_).cumprod()\n",
    "            stats[name].loc[numIter] = p_.iloc[-1] - 1  # terminal return\n",
    "\n",
    "    # 5) Report results\n",
    "    stats = pd.DataFrame.from_dict(stats, orient='columns')\n",
    "    # stats.to_csv('stats.csv')\n",
    "    df0, df1 = stats.std(), stats.var()\n",
    "    print pd.concat([df0, df1, df1 / df1['getHRP'] - 1], axis=1)\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = [u'EEM', u'EWG', u'TIP', u'EWJ', u'EFA', u'IEF', u'EWQ', \n",
    "           u'EWU', u'XLB', u'XLE', u'XLF', u'LQD', u'XLK', u'XLU', \n",
    "           u'EPP', u'FXI', u'VGK', u'VPL', u'SPY', u'TLT', u'BND', \n",
    "           u'CSJ', u'DIA']\n",
    "#rets = (get_pricing(symbols, \n",
    " #                   fields='price', \n",
    "  #                  start_date='2008-1-1',\n",
    "   #                 end_date='2016-7-1',\n",
    "    #               )\n",
    "     #   .pct_change()\n",
    "      #  .rename(columns=lambda x: x.symbol)\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'local_csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-82fed88eacc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnetwork_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocal_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Quantopian.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_column\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnetwork_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnetwork_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpivot_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'symbol'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weight'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ffill'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'local_csv' is not defined"
     ]
    }
   ],
   "source": [
    "network_weights = local_csv('Quantopian.csv', date_column = 'date')\n",
    "network_weights = (network_weights.reset_index().pivot_table(columns='symbol', values='weight', index='date').resample('b', fill_method='ffill'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'local_csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-7becafee98dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mnetwork_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocal_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Quantopian.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_column\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mnetwork_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnetwork_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpivot_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'symbol'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weight'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ffill'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'local_csv' is not defined"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    network_weights = local_csv('Quantopian.csv', date_column = 'date')\n",
    "    network_weights = (network_weights.reset_index().pivot_table(columns='symbol', values='weight', index='date').resample('b', fill_method='ffill'))\n",
    "except IOError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-86afc587e2d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# There has to be a more succinct way to do this using rolling_apply or resample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Would love to see a better version of this.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0meoms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'1BM'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mcovs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPanel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meoms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminor_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmajor_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcorrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPanel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meoms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminor_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmajor_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rets' is not defined"
     ]
    }
   ],
   "source": [
    "# There has to be a more succinct way to do this using rolling_apply or resample\n",
    "# Would love to see a better version of this.\n",
    "eoms = rets.resample('1BM').index[13:-1]\n",
    "covs = pd.Panel(items=eoms, minor_axis=rets.columns, major_axis=rets.columns)\n",
    "corrs = pd.Panel(items=eoms, minor_axis=rets.columns, major_axis=rets.columns)\n",
    "covs_robust = pd.Panel(items=eoms, minor_axis=rets.columns, major_axis=rets.columns)\n",
    "corrs_robust = pd.Panel(items=eoms, minor_axis=rets.columns, major_axis=rets.columns)\n",
    "for eom in eoms:\n",
    "    covs.loc[eom] = rets.loc[eom-pd.Timedelta('252d'):eom].cov()\n",
    "    corrs.loc[eom] = rets.loc[eom-pd.Timedelta('252d'):eom].corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "portfolio_funcs = OrderedDict((\n",
    "    ('Equal weighting', lambda returns, cov, corr: np.ones(cov.shape[0]) / len(cov.columns)),\n",
    "    ('Inverse Variance weighting', lambda returns, cov, corr: getIVP(cov)),\n",
    "    #('Minimum-variance (CLA) weighting', getCLA),\n",
    "    ('Mean-Variance weighting', lambda returns, cov, corr: get_mean_variance(returns, cov)),\n",
    "    ('Robust Mean-Variance weighting', lambda returns, cov, corr: get_mean_variance(returns, cov=cov_robust(cov))),\n",
    "    ('Min-Variance weighting', lambda returns, cov, corr: get_min_variance(returns, cov)),\n",
    "    ('Robust Min-Variance weighting', lambda returns, cov, corr: get_min_variance(returns, cov=cov_robust(cov))),        \n",
    "    ('Hierarchical weighting (by LdP)', lambda returns, cov, corr: getHRP(cov, corr)),\n",
    "    ('Robust Hierarchical weighting (by LdP)', lambda returns, cov, corr: getHRP(cov_robust(cov), corr_robust(corr))),\n",
    "    #('Network weighting (by Jochen Papenbrock)', lambda x: network_weights.loc[x])\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = pd.Panel(items=portfolio_funcs.keys(), major_axis=eoms, minor_axis=symbols, dtype=np.float32)\n",
    "port_returns = pd.DataFrame(columns=portfolio_funcs.keys(), index=rets.index)\n",
    "for name, portfolio_func in portfolio_funcs.iteritems():\n",
    "    w = pd.DataFrame(index=eoms, columns=symbols, dtype=np.float32)\n",
    "    for idx in covs:\n",
    "        w.loc[idx] = portfolio_func(rets.loc[idx-pd.Timedelta('252d'):idx].T, \n",
    "                                    covs.loc[idx],\n",
    "                                    corrs.loc[idx]\n",
    "        )\n",
    "    \n",
    "    port_returns[name] = w.loc[rets.index].ffill().multiply(rets).sum(axis='columns')\n",
    "\n",
    "    weights.loc[name, :, :] = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.clustermap(rets.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=3, figsize=(10, 22))\n",
    "colors = sns.color_palette(palette='Set3', n_colors=len(port_returns))\n",
    "\n",
    "for i, name in enumerate(port_returns):\n",
    "    np.log1p(port_returns[name]).cumsum().plot(ax=axs[0], color=colors[i])\n",
    "    \n",
    "axs[0].legend(loc=0)\n",
    "axs[0].set(ylabel='Cumulative log returns')\n",
    "\n",
    "sharpes = port_returns.apply(pf.timeseries.sharpe_ratio)\n",
    "sns.barplot(x=sharpes.values, y=sharpes.index, ax=axs[1])\n",
    "axs[1].set(xlabel='Sharpe ratio')\n",
    "\n",
    "vols = port_returns.apply(pf.timeseries.annual_volatility)\n",
    "sns.barplot(x=vols.values, y=vols.index, ax=axs[2])\n",
    "axs[2].set(xlabel='Annual volatility')\n",
    "\n",
    "#cal = port_returns.apply(pf.timeseries.calmar_ratio)\n",
    "#sns.barplot(x=cal.values, y=cal.index, ax=ax4)\n",
    "#ax4.set(xlabel='Calmar Ratio')\n",
    "\n",
    "#tr = port_returns.apply(tail_ratio)\n",
    "#sns.barplot(x=tr.values, y=tr.index, ax=ax5)\n",
    "#ax5.set(xlabel='Tail Ratio') \n",
    "\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
